{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "batch-sizes-comparison.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hfnzyvaY4MN-"
      },
      "source": [
        "This first block of text simply contains all the libraries that need importing as well as initialising some variables of this example. You may change the batch_size and epochs variables, but should not change num_classes, img_rows and img_cols as these depend on the dataset being used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eE1eAkQo4MOE"
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.datasets as datasets\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_sizes = [32, 64, 128, 256, 512]\n",
        "num_classes = 10\n",
        "epochs = 30\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 32, 32"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Zv0V2lBn4MOF"
      },
      "source": [
        "The next block of text handles all the data loading (in this case of the CIFAR-10 dataset) and reshaping so that it can be used to train and evaluate the CNN models. Two sets of data are loaded, the training data, used to generate the model, and the test data, used to evaluate if this model is good at making predictions\n",
        "\n",
        "The final set of code of this cell defines the data augmentation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdNwqIZk4MOG",
        "outputId": "c85073fa-06a0-4fec-b777-81bd2285fec5"
      },
      "source": [
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
        "    input_shape = (3, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
        "    input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_orig = y_test\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Enable the data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mrGtxIti4MOG"
      },
      "source": [
        "This cell loads given pre-defined architecture, that had been trained for a different problem. It removes the last layer (because that would be specific to the original problem, and adds some new layers. This process of adapting a network trained on a different problem is called *Transfer Learning*.\n",
        "\n",
        "The only essential new layer to add is the softmax one (last) that specifies the number of classes of the new dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EPtEKuBt4MOG"
      },
      "source": [
        "def get_model():\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    # add a global spatial average pooling layer\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # let's add a fully-connected layer\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    # and a logistic layer -- let's say we have 200 classes\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # The rest of this cell is common to both defining the full architecture or using a pre-trained one\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adamax(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qzRqYiif4MOH"
      },
      "source": [
        "This is the block of code that trains models and evaluates their predictive capacity on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bMsKlOx4MOH",
        "outputId": "765ea80b-add5-40f8-f566-27d857e4e10c"
      },
      "source": [
        "# Enable Early Stopping to prevent model from overfitting\n",
        "callback=keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "results = []\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    model = get_model()\n",
        "\n",
        "    # Fits the model on batches with real-time data augmentation\n",
        "    history = model.fit(datagen.flow(x_train, y_train,\n",
        "             batch_size=batch_size),\n",
        "             steps_per_epoch=len(x_train) / batch_size,\n",
        "             epochs=epochs,\n",
        "             callbacks=[callback],\n",
        "             validation_data=(x_test, y_test),\n",
        "             verbose=1)\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    epoch = history.history['val_loss'].index(val_loss)\n",
        "    val_accuracy = history.history['val_accuracy'][epoch]\n",
        "\n",
        "    # Append model results\n",
        "    results.append([batch_size, '%.2f%%' % (val_accuracy * 100), '%.2f' % val_loss, epoch + 1])\n",
        "\n",
        "    # Display the best accuracy and loss of the CNN architecture\n",
        "    print('Batch size:', results[-1][0], ', Accuracy:', results[-1][1], ', Loss:', results[-1][2])\n",
        "    print()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Epoch 1/30\n",
            "1562/1562 [==============================] - 140s 70ms/step - loss: 1.9177 - accuracy: 0.2579 - val_loss: 0.9654 - val_accuracy: 0.6637\n",
            "Epoch 2/30\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.9504 - accuracy: 0.6738 - val_loss: 0.6402 - val_accuracy: 0.7857\n",
            "Epoch 3/30\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.6920 - accuracy: 0.7711 - val_loss: 0.6224 - val_accuracy: 0.8012\n",
            "Epoch 4/30\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 0.5724 - accuracy: 0.8113 - val_loss: 0.7588 - val_accuracy: 0.7787\n",
            "Epoch 5/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.4982 - accuracy: 0.8352 - val_loss: 0.5092 - val_accuracy: 0.8334\n",
            "Epoch 6/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.4365 - accuracy: 0.8555 - val_loss: 0.5192 - val_accuracy: 0.8390\n",
            "Epoch 7/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.3891 - accuracy: 0.8700 - val_loss: 0.4788 - val_accuracy: 0.8501\n",
            "Epoch 8/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.3490 - accuracy: 0.8841 - val_loss: 0.4327 - val_accuracy: 0.8589\n",
            "Epoch 9/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.3074 - accuracy: 0.8964 - val_loss: 0.4774 - val_accuracy: 0.8531\n",
            "Epoch 10/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.2815 - accuracy: 0.9082 - val_loss: 0.4508 - val_accuracy: 0.8670\n",
            "Epoch 11/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.2616 - accuracy: 0.9134 - val_loss: 0.4272 - val_accuracy: 0.8752\n",
            "Epoch 12/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.2393 - accuracy: 0.9214 - val_loss: 0.3969 - val_accuracy: 0.8781\n",
            "Epoch 13/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.2150 - accuracy: 0.9301 - val_loss: 0.4671 - val_accuracy: 0.8598\n",
            "Epoch 14/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.1994 - accuracy: 0.9337 - val_loss: 0.4134 - val_accuracy: 0.8861\n",
            "Epoch 15/30\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.1842 - accuracy: 0.9385 - val_loss: 0.4004 - val_accuracy: 0.8824\n",
            "Epoch 16/30\n",
            "1562/1562 [==============================] - 105s 67ms/step - loss: 0.1658 - accuracy: 0.9456 - val_loss: 0.4205 - val_accuracy: 0.8808\n",
            "Epoch 17/30\n",
            "1562/1562 [==============================] - 105s 67ms/step - loss: 0.1561 - accuracy: 0.9496 - val_loss: 0.4393 - val_accuracy: 0.8853\n",
            "Batch size: 32 , Accuracy: 87.81% , Loss: 0.40\n",
            "\n",
            "Epoch 1/30\n",
            "781/781 [==============================] - 72s 89ms/step - loss: 1.9189 - accuracy: 0.2520 - val_loss: 1.0023 - val_accuracy: 0.6433\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 69s 89ms/step - loss: 0.9609 - accuracy: 0.6601 - val_loss: 0.7157 - val_accuracy: 0.7661\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 69s 89ms/step - loss: 0.7177 - accuracy: 0.7624 - val_loss: 0.6707 - val_accuracy: 0.7790\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 69s 89ms/step - loss: 0.5889 - accuracy: 0.8053 - val_loss: 0.5295 - val_accuracy: 0.8330\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 69s 89ms/step - loss: 0.5025 - accuracy: 0.8351 - val_loss: 0.6084 - val_accuracy: 0.8053\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 69s 89ms/step - loss: 0.4387 - accuracy: 0.8551 - val_loss: 0.5795 - val_accuracy: 0.8285\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.3990 - accuracy: 0.8692 - val_loss: 0.4697 - val_accuracy: 0.8483\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.3572 - accuracy: 0.8810 - val_loss: 0.4707 - val_accuracy: 0.8557\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.3141 - accuracy: 0.8953 - val_loss: 0.4757 - val_accuracy: 0.8554\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.2872 - accuracy: 0.9063 - val_loss: 0.4904 - val_accuracy: 0.8557\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 69s 89ms/step - loss: 0.2649 - accuracy: 0.9123 - val_loss: 0.4465 - val_accuracy: 0.8648\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.2461 - accuracy: 0.9187 - val_loss: 0.4101 - val_accuracy: 0.8759\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.2143 - accuracy: 0.9286 - val_loss: 0.4308 - val_accuracy: 0.8679\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.2015 - accuracy: 0.9346 - val_loss: 0.4440 - val_accuracy: 0.8747\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.1916 - accuracy: 0.9375 - val_loss: 0.4047 - val_accuracy: 0.8789\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.1760 - accuracy: 0.9412 - val_loss: 0.4213 - val_accuracy: 0.8791\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.1670 - accuracy: 0.9453 - val_loss: 0.4475 - val_accuracy: 0.8781\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 69s 88ms/step - loss: 0.1499 - accuracy: 0.9493 - val_loss: 0.4968 - val_accuracy: 0.8672\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 69s 89ms/step - loss: 0.1454 - accuracy: 0.9521 - val_loss: 0.4643 - val_accuracy: 0.8710\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 70s 89ms/step - loss: 0.1250 - accuracy: 0.9577 - val_loss: 0.4449 - val_accuracy: 0.8831\n",
            "Batch size: 64 , Accuracy: 87.89% , Loss: 0.40\n",
            "\n",
            "Epoch 1/30\n",
            "390/390 [==============================] - 59s 143ms/step - loss: 2.0842 - accuracy: 0.1849 - val_loss: 1.3189 - val_accuracy: 0.4843\n",
            "Epoch 2/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 1.2221 - accuracy: 0.5403 - val_loss: 0.7988 - val_accuracy: 0.7214\n",
            "Epoch 3/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.8063 - accuracy: 0.7197 - val_loss: 0.7241 - val_accuracy: 0.7528\n",
            "Epoch 4/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.6376 - accuracy: 0.7830 - val_loss: 0.6303 - val_accuracy: 0.7887\n",
            "Epoch 5/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.5475 - accuracy: 0.8190 - val_loss: 0.5094 - val_accuracy: 0.8319\n",
            "Epoch 6/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.4615 - accuracy: 0.8463 - val_loss: 0.4952 - val_accuracy: 0.8429\n",
            "Epoch 7/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.4341 - accuracy: 0.8548 - val_loss: 0.4778 - val_accuracy: 0.8481\n",
            "Epoch 8/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.3761 - accuracy: 0.8750 - val_loss: 0.4402 - val_accuracy: 0.8569\n",
            "Epoch 9/30\n",
            "390/390 [==============================] - 54s 138ms/step - loss: 0.3434 - accuracy: 0.8871 - val_loss: 0.4304 - val_accuracy: 0.8600\n",
            "Epoch 10/30\n",
            "390/390 [==============================] - 54s 138ms/step - loss: 0.3175 - accuracy: 0.8931 - val_loss: 0.4312 - val_accuracy: 0.8698\n",
            "Epoch 11/30\n",
            "390/390 [==============================] - 54s 138ms/step - loss: 0.2793 - accuracy: 0.9059 - val_loss: 0.4531 - val_accuracy: 0.8593\n",
            "Epoch 12/30\n",
            "390/390 [==============================] - 54s 138ms/step - loss: 0.2591 - accuracy: 0.9142 - val_loss: 0.4059 - val_accuracy: 0.8765\n",
            "Epoch 13/30\n",
            "390/390 [==============================] - 54s 138ms/step - loss: 0.2439 - accuracy: 0.9192 - val_loss: 0.4314 - val_accuracy: 0.8741\n",
            "Epoch 14/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.2198 - accuracy: 0.9271 - val_loss: 0.3835 - val_accuracy: 0.8818\n",
            "Epoch 15/30\n",
            "390/390 [==============================] - 54s 138ms/step - loss: 0.1968 - accuracy: 0.9348 - val_loss: 0.4264 - val_accuracy: 0.8745\n",
            "Epoch 16/30\n",
            "390/390 [==============================] - 54s 138ms/step - loss: 0.1771 - accuracy: 0.9412 - val_loss: 0.4217 - val_accuracy: 0.8795\n",
            "Epoch 17/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.1620 - accuracy: 0.9463 - val_loss: 0.3983 - val_accuracy: 0.8845\n",
            "Epoch 18/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.1561 - accuracy: 0.9467 - val_loss: 0.4488 - val_accuracy: 0.8749\n",
            "Epoch 19/30\n",
            "390/390 [==============================] - 54s 137ms/step - loss: 0.1355 - accuracy: 0.9550 - val_loss: 0.4353 - val_accuracy: 0.8763\n",
            "Batch size: 128 , Accuracy: 88.18% , Loss: 0.38\n",
            "\n",
            "Epoch 1/30\n",
            "195/195 [==============================] - 52s 239ms/step - loss: 2.2055 - accuracy: 0.1505 - val_loss: 1.6602 - val_accuracy: 0.3409\n",
            "Epoch 2/30\n",
            "195/195 [==============================] - 46s 238ms/step - loss: 1.5196 - accuracy: 0.4090 - val_loss: 1.1149 - val_accuracy: 0.5826\n",
            "Epoch 3/30\n",
            "195/195 [==============================] - 46s 235ms/step - loss: 1.0495 - accuracy: 0.6156 - val_loss: 0.7680 - val_accuracy: 0.7342\n",
            "Epoch 4/30\n",
            "195/195 [==============================] - 46s 233ms/step - loss: 0.7779 - accuracy: 0.7304 - val_loss: 0.7315 - val_accuracy: 0.7603\n",
            "Epoch 5/30\n",
            "195/195 [==============================] - 46s 234ms/step - loss: 0.6426 - accuracy: 0.7823 - val_loss: 0.5343 - val_accuracy: 0.8200\n",
            "Epoch 6/30\n",
            "195/195 [==============================] - 45s 230ms/step - loss: 0.5464 - accuracy: 0.8139 - val_loss: 0.6397 - val_accuracy: 0.7963\n",
            "Epoch 7/30\n",
            "195/195 [==============================] - 46s 234ms/step - loss: 0.4821 - accuracy: 0.8398 - val_loss: 0.5278 - val_accuracy: 0.8267\n",
            "Epoch 8/30\n",
            "195/195 [==============================] - 46s 236ms/step - loss: 0.4425 - accuracy: 0.8515 - val_loss: 0.4695 - val_accuracy: 0.8414\n",
            "Epoch 9/30\n",
            "195/195 [==============================] - 46s 235ms/step - loss: 0.3903 - accuracy: 0.8683 - val_loss: 0.4368 - val_accuracy: 0.8571\n",
            "Epoch 10/30\n",
            "195/195 [==============================] - 45s 232ms/step - loss: 0.3472 - accuracy: 0.8836 - val_loss: 0.4272 - val_accuracy: 0.8569\n",
            "Epoch 11/30\n",
            "195/195 [==============================] - 46s 233ms/step - loss: 0.3226 - accuracy: 0.8919 - val_loss: 0.4132 - val_accuracy: 0.8659\n",
            "Epoch 12/30\n",
            "195/195 [==============================] - 46s 233ms/step - loss: 0.2922 - accuracy: 0.9004 - val_loss: 0.4700 - val_accuracy: 0.8535\n",
            "Epoch 13/30\n",
            "195/195 [==============================] - 46s 233ms/step - loss: 0.2662 - accuracy: 0.9096 - val_loss: 0.4471 - val_accuracy: 0.8562\n",
            "Epoch 14/30\n",
            "195/195 [==============================] - 46s 234ms/step - loss: 0.2434 - accuracy: 0.9173 - val_loss: 0.4476 - val_accuracy: 0.8640\n",
            "Epoch 15/30\n",
            "195/195 [==============================] - 46s 234ms/step - loss: 0.2177 - accuracy: 0.9264 - val_loss: 0.4353 - val_accuracy: 0.8699\n",
            "Epoch 16/30\n",
            "195/195 [==============================] - 46s 236ms/step - loss: 0.2085 - accuracy: 0.9298 - val_loss: 0.4308 - val_accuracy: 0.8699\n",
            "Batch size: 256 , Accuracy: 86.59% , Loss: 0.41\n",
            "\n",
            "Epoch 1/30\n",
            "97/97 [==============================] - 60s 506ms/step - loss: 2.2262 - accuracy: 0.1398 - val_loss: 1.8401 - val_accuracy: 0.2507\n",
            "Epoch 2/30\n",
            "97/97 [==============================] - 44s 445ms/step - loss: 1.8018 - accuracy: 0.2606 - val_loss: 1.4458 - val_accuracy: 0.4231\n",
            "Epoch 3/30\n",
            "97/97 [==============================] - 44s 444ms/step - loss: 1.4268 - accuracy: 0.4385 - val_loss: 1.1488 - val_accuracy: 0.5713\n",
            "Epoch 4/30\n",
            "97/97 [==============================] - 44s 445ms/step - loss: 1.1107 - accuracy: 0.5832 - val_loss: 0.8717 - val_accuracy: 0.6875\n",
            "Epoch 5/30\n",
            "97/97 [==============================] - 44s 449ms/step - loss: 0.9210 - accuracy: 0.6707 - val_loss: 0.8296 - val_accuracy: 0.7104\n",
            "Epoch 6/30\n",
            "97/97 [==============================] - 45s 455ms/step - loss: 0.7658 - accuracy: 0.7307 - val_loss: 0.6349 - val_accuracy: 0.7842\n",
            "Epoch 7/30\n",
            "97/97 [==============================] - 44s 447ms/step - loss: 0.6276 - accuracy: 0.7859 - val_loss: 0.5857 - val_accuracy: 0.8016\n",
            "Epoch 8/30\n",
            "97/97 [==============================] - 44s 445ms/step - loss: 0.5587 - accuracy: 0.8034 - val_loss: 0.5280 - val_accuracy: 0.8192\n",
            "Epoch 9/30\n",
            "97/97 [==============================] - 44s 444ms/step - loss: 0.5042 - accuracy: 0.8273 - val_loss: 0.4835 - val_accuracy: 0.8359\n",
            "Epoch 10/30\n",
            "97/97 [==============================] - 44s 447ms/step - loss: 0.4524 - accuracy: 0.8460 - val_loss: 0.4839 - val_accuracy: 0.8365\n",
            "Epoch 11/30\n",
            "97/97 [==============================] - 43s 443ms/step - loss: 0.4077 - accuracy: 0.8610 - val_loss: 0.4529 - val_accuracy: 0.8471\n",
            "Epoch 12/30\n",
            "97/97 [==============================] - 43s 443ms/step - loss: 0.3780 - accuracy: 0.8693 - val_loss: 0.4644 - val_accuracy: 0.8452\n",
            "Epoch 13/30\n",
            "97/97 [==============================] - 43s 444ms/step - loss: 0.3483 - accuracy: 0.8819 - val_loss: 0.4347 - val_accuracy: 0.8580\n",
            "Epoch 14/30\n",
            "97/97 [==============================] - 43s 444ms/step - loss: 0.3229 - accuracy: 0.8904 - val_loss: 0.4424 - val_accuracy: 0.8562\n",
            "Epoch 15/30\n",
            "97/97 [==============================] - 43s 442ms/step - loss: 0.2878 - accuracy: 0.8997 - val_loss: 0.4704 - val_accuracy: 0.8508\n",
            "Epoch 16/30\n",
            "97/97 [==============================] - 43s 442ms/step - loss: 0.2706 - accuracy: 0.9067 - val_loss: 0.4264 - val_accuracy: 0.8663\n",
            "Epoch 17/30\n",
            "97/97 [==============================] - 44s 446ms/step - loss: 0.2455 - accuracy: 0.9154 - val_loss: 0.4011 - val_accuracy: 0.8710\n",
            "Epoch 18/30\n",
            "97/97 [==============================] - 43s 441ms/step - loss: 0.2247 - accuracy: 0.9227 - val_loss: 0.4288 - val_accuracy: 0.8625\n",
            "Epoch 19/30\n",
            "97/97 [==============================] - 43s 441ms/step - loss: 0.2205 - accuracy: 0.9253 - val_loss: 0.4423 - val_accuracy: 0.8689\n",
            "Epoch 20/30\n",
            "97/97 [==============================] - 44s 446ms/step - loss: 0.1961 - accuracy: 0.9331 - val_loss: 0.4289 - val_accuracy: 0.8710\n",
            "Epoch 21/30\n",
            "97/97 [==============================] - 44s 446ms/step - loss: 0.1704 - accuracy: 0.9419 - val_loss: 0.4321 - val_accuracy: 0.8702\n",
            "Epoch 22/30\n",
            "97/97 [==============================] - 44s 448ms/step - loss: 0.1768 - accuracy: 0.9393 - val_loss: 0.4303 - val_accuracy: 0.8714\n",
            "Batch size: 512 , Accuracy: 87.10% , Loss: 0.40\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GJKrHica4MOI"
      },
      "source": [
        "Finally, comparison table of models with different batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "KZ0Fl9q44MOI",
        "outputId": "2f9277ea-7dac-4a0d-f57c-ffebd896f132"
      },
      "source": [
        "# Display table\n",
        "plt.figure(figsize=(7,1.5))\n",
        "plt.table(colLabels=['Batch size', 'Accuracy', 'Loss', 'Epochs'],\n",
        "          cellText=results,\n",
        "          cellLoc='center',\n",
        "          loc='center')\n",
        "plt.axis('off')\n",
        "plt.savefig('Batch-sizes-comparison.jpg', bbox_inches='tight', pad_inches=0.1, dpi=200)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x108 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}